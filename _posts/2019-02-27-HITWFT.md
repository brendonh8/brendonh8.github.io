---
title: "How is the World Feeling Today? - Part One"
date: 2019-02-27
tags: [Python, AWS, Scala, SQL, Django]
header:
    overlay_image: "/assets/images/hitwft/globe.jpg"
excerpt: "A web application showing the sentiment of news stories all over the globe."
---
## Table of Contents

- [Overview](#heading-1)

- [Bootstrapping](#heading-1)

## <a name="heading-1"></a>Overview

Checking the news is limited to what location you are in and usually what the top stories of the day are for the source you enjoy looking at. What if news could be viewed in a different way? I thought it would be an interesting change if you could find news stories by looking at the overall sentiment in an area. Color gradients for sentiment could be shown for cities all around the globe so a user could select an area that looks highly positive or negative and see what top news stories might be causing this. I accomplished this using the very detailed database called GDELT.

GDELT, or the Global Database of Events, Language, and Tone, is a conglomeration of news stories and events dating back to 1979. GDELT summarizes over 300 categories, 60 attributes and thousands of emotions for millions of different themes. There are so many different aspects of this dataset that I can't explain them all. If you are interested, check out the source information at [here](https://www.gdeltproject.org/). 

The portion of the dataset that I focused on is called the GKG, or Global Knowledge Graph. GKG reads the emotions from every news report using some of the most sophisticated named entity and geocoding algorithms in existance. I am using this emotion data to sum sentiments of news stories by cities to overlay on an interactive map web app. This post will go through the pipeline I used to feed the data from the GDELT dataset into a postgGIS database. Part two will show how I took the data from the database with django and created an interactive web app.

## <a name="heading-1"></a>Bootstrapping

There are countless choices when putting together a data pipeline since there are so many different tools you can use. Initially I was going to use lambda triggers in AWS to export the data I needed into a postgres database for further querying. I decided this might not be robust enough for anything I decide to do with the data down the line.

Taking a slightly Agile approach, I wanted to just get an MVP app out so I started focusing on just getting the sentiment points onto an interactive map. Once that was done I could further add to the pipeline and start extracting any other data I needed from the vast GDELT database.

The main tool I used for the project was Airflow. It let me easily create a skeleton pipeline first and then later on I can add what I need with little changes. Since I was most likely going to do further computations other than adding sentiment, I wanted the project to be scalable. I decided to use AWS EMR clusters to run spark jobs for any transformations I needed. Although, I didn't want these EMR clusters to run all the time so I automated launching and terminating instances when computation was required.

<figure>
	<img src="/assets/images/hitwft/cloudformation.png">
	<figcaption>Cloudformation makes it easy to start up multiple different pipeline resources at once.</figcaption>
</figure>

I found a pretty useful tool that AWS has called Cloudformation. Cloudformation is basically just a yaml file that you can define all of the resources you want to use and AWS will spin them up in the correct order of dependencies. There are many templates floating around so I found one that started an EC2 instance and an RDS database.

